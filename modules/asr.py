import time
import numpy as np
from queue import Queue, Empty
from PyQt6.QtCore import QThread, pyqtSignal
from faster_whisper import WhisperModel
from config import Config


class ASRWorker(QThread):
    text_recognized = pyqtSignal(str)

    def __init__(self, audio_queue: Queue):
        super().__init__()
        self.audio_queue = audio_queue
        self.running = True
        self.model = None
        
        # æ–‡æœ¬ç¼“å†²ï¼ˆç”¨äºåˆå¹¶ç‰‡æ®µï¼‰
        self.text_buffer = ""
        self.last_emit_time = 0

    def run(self):
        print(f"[Whisper] åŠ è½½æ¨¡å‹: {Config.WHISPER_MODEL_SIZE}...")
        try:
            self.model = WhisperModel(
                Config.WHISPER_MODEL_SIZE,
                device=Config.DEVICE,
                compute_type=Config.COMPUTE_TYPE
            )
            print("[Whisper] âœ… æ¨¡å‹å°±ç»ª")
        except Exception as e:
            print(f"[Whisper] âŒ åŠ è½½å¤±è´¥: {e}")
            return

        audio_buffer = np.array([], dtype=np.float32)
        
        # é˜ˆå€¼è®¡ç®—
        min_chunk_samples = int(Config.SAMPLE_RATE * Config.VAD_CHUNK_SECONDS)
        max_chunk_samples = int(Config.SAMPLE_RATE * Config.MAX_BUFFER_SECONDS)
        
        buffer_start_time = time.time()
        silence_start_time = None
        
        # é™éŸ³æ£€æµ‹é˜ˆå€¼ï¼ˆRMS èƒ½é‡ï¼‰
        SILENCE_THRESHOLD = 0.01
        SILENCE_DURATION_FOR_SPLIT = Config.MIN_SILENCE_DURATION_MS / 1000.0

        debug_counter = 0
        
        while self.running:
            try:
                chunk = self.audio_queue.get(timeout=0.5)
                audio_buffer = np.concatenate((audio_buffer, chunk))
                
                # æ£€æµ‹å½“å‰ chunk æ˜¯å¦ä¸ºé™éŸ³
                chunk_rms = np.sqrt(np.mean(chunk ** 2))
                is_silence = chunk_rms < SILENCE_THRESHOLD
                
                # è°ƒè¯•æ—¥å¿—ï¼šæ¯10æ¬¡å¤„ç†æ‰“å°ä¸€æ¬¡çŠ¶æ€
                debug_counter += 1
                if debug_counter % 10 == 0:
                    buffer_sec = len(audio_buffer) / Config.SAMPLE_RATE
                    print(f"[ASR] ğŸ“Š DEBUG: chunk_rms={chunk_rms:.6f}, é™éŸ³={is_silence}, ç¼“å†²={buffer_sec:.1f}s, é˜ˆå€¼={SILENCE_THRESHOLD}")
                
                current_time = time.time()
                buffer_duration = len(audio_buffer) / Config.SAMPLE_RATE
                
                # é™éŸ³è®¡æ—¶
                if is_silence:
                    if silence_start_time is None:
                        silence_start_time = current_time
                    silence_duration = current_time - silence_start_time
                else:
                    silence_start_time = None
                    silence_duration = 0
                
                # å†³å®šæ˜¯å¦è§¦å‘è¯†åˆ«çš„æ¡ä»¶
                should_transcribe = False
                reason = ""
                
                # æ¡ä»¶1: æ£€æµ‹åˆ°è¶³å¤Ÿé•¿çš„é™éŸ³ ä¸” ç¼“å†²åŒºæœ‰è¶³å¤Ÿå†…å®¹
                if silence_duration >= SILENCE_DURATION_FOR_SPLIT and len(audio_buffer) >= min_chunk_samples:
                    should_transcribe = True
                    reason = f"é™éŸ³ {silence_duration:.1f}s"
                
                # æ¡ä»¶2: ç¼“å†²åŒºè¶…è¿‡æœ€å¤§é•¿åº¦ï¼ˆå…œåº•ï¼‰
                elif len(audio_buffer) >= max_chunk_samples:
                    should_transcribe = True
                    reason = f"ç¼“å†²åŒºæ»¡ {buffer_duration:.1f}s"
                
                if should_transcribe:
                    print(f"[ASR] è§¦å‘è¯†åˆ« ({reason})")
                    self._transcribe(audio_buffer)
                    audio_buffer = np.array([], dtype=np.float32)
                    buffer_start_time = time.time()
                    silence_start_time = None
                    
            except Empty:
                # è¶…æ—¶ä½†ç¼“å†²åŒºæœ‰å†…å®¹ï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥å¤„ç†
                if len(audio_buffer) > 0:
                    elapsed = time.time() - buffer_start_time
                    # å¦‚æœè¶…è¿‡æœ€å¤§ç­‰å¾…æ—¶é—´çš„ä¸€åŠï¼Œä¸”æœ‰å†…å®¹ï¼Œå¤„ç†å®ƒ
                    if elapsed > Config.MAX_BUFFER_SECONDS / 2 and len(audio_buffer) >= min_chunk_samples // 2:
                        print(f"[ASR] è¶…æ—¶å¤„ç† ({elapsed:.1f}s)")
                        self._transcribe(audio_buffer)
                        audio_buffer = np.array([], dtype=np.float32)
                        buffer_start_time = time.time()
                continue
            except Exception as e:
                print(f"[ASR] é”™è¯¯: {e}")

    def _transcribe(self, audio_data):
        if not self.model:
            return

        try:
            # ä½¿ç”¨å†…ç½® VAD è¿‡æ»¤
            segments, info = self.model.transcribe(
                audio_data,
                beam_size=1,
                language="en",
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=Config.MIN_SILENCE_DURATION_MS)
            )

            text = " ".join([s.text for s in segments]).strip()
            
            if text:
                # æ™ºèƒ½åˆå¹¶ï¼šå¦‚æœä¸Šæ¬¡å‘é€ä¸ä¹…ä¸”æ–‡æœ¬è¾ƒçŸ­ï¼Œå°è¯•åˆå¹¶
                current_time = time.time()
                time_since_last = current_time - self.last_emit_time
                
                # å¦‚æœè·ç¦»ä¸Šæ¬¡å‘é€ä¸åˆ°2ç§’ï¼Œä¸”æ–‡æœ¬çœ‹èµ·æ¥ä¸å®Œæ•´ï¼Œç´¯ç§¯
                if time_since_last < 2.0 and not self._looks_complete(self.text_buffer) and len(self.text_buffer) < 80:
                    self.text_buffer = (self.text_buffer + " " + text).strip()
                    print(f"[ASR] ğŸ“ ç´¯ç§¯: {self.text_buffer}")
                else:
                    # å‘é€ä¹‹å‰ç´¯ç§¯çš„å†…å®¹
                    if self.text_buffer:
                        combined = (self.text_buffer + " " + text).strip()
                        print(f"[ASR] ğŸ‘‚: {combined}")
                        self.text_recognized.emit(combined)
                        self.text_buffer = ""
                    else:
                        print(f"[ASR] ğŸ‘‚: {text}")
                        self.text_recognized.emit(text)
                    
                    self.last_emit_time = current_time
                    
        except Exception as e:
            print(f"[ASR] è¯†åˆ«é”™è¯¯: {e}")
    
    def _looks_complete(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦çœ‹èµ·æ¥å®Œæ•´ï¼ˆä»¥å¥å­ç»“æŸç¬¦ç»“å°¾ï¼‰"""
        if not text:
            return True
        text = text.rstrip()
        return text.endswith(('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '"', "'"))

    def stop(self):
        self.running = False
        # æ¸…ç©ºæ®‹ä½™ç¼“å†²
        if self.text_buffer:
            print(f"[ASR] ğŸ‘‚ (ç»“æŸ): {self.text_buffer}")
            self.text_recognized.emit(self.text_buffer)
            self.text_buffer = ""
